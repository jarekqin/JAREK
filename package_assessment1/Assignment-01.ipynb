{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lesson-01 Assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`各位同学大家好，欢迎各位开始学习我们的人工智能课程。这门课程假设大家不具备机器学习和人工智能的知识，但是希望大家具备初级的Python编程能力。根据往期同学的实际反馈，我们课程的完结之后 能力能够超过80%的计算机人工智能/深度学习方向的硕士生的能力。`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 本次作业的内容"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. 复现课堂代码\n",
    "\n",
    "在本部分，你需要参照我们给大家的GitHub地址里边的课堂代码，结合课堂内容，复现内容。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. 请回答以下问题\n",
    "\n",
    "回答以下问题，并将问题发送至 mqgao@kaikeba.com中：\n",
    "```\n",
    "    2.1. what do you want to acquire in this course？\n",
    "    2.2. what problems do you want to solve？\n",
    "    2.3. what’s the advantages you have to finish you goal?\n",
    "    2.4. what’s the disadvantages you need to overcome to finish you goal?\n",
    "    2.5. How will you plan to study in this course period?\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. 如何提交\n",
    "代码 + 此 jupyter 相关，提交至自己的 github 中(**所以请务必把GitHub按照班主任要求录入在Trello中**)；\n",
    "第2问，请提交至mqgao@kaikeba.com邮箱。\n",
    "#### 4. 作业截止时间\n",
    "此次作业截止时间为 2019.7.6日"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. 完成以下问答和编程练习"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 基础理论部分"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 0. Can you come up out 3 sceneraies which use AI methods? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans: Bank verified system which can verify automatically with different loan applyer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. How do we use Github; Why do we use Jupyter and Pycharm;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans: With daily working to store our codes as a coding store. Also, we can invite someones to enjoy our open-souuced projects, as well as sharing our project, files and documents to others.Pychar for developing, and jupyter for demonstrating.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. What's the Probability Model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans: With sets of strings that are given probability to create words or sentences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Can you came up with some sceneraies at which we could use Probability Model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans:Giving closed answer to customer who is using online customer service;\n",
    "    Solving some small problems with sending message;\n",
    "    Self-service provided by ATM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Why do we use probability and what's the difficult points for programming based on parsing and pattern match?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans: Probability pattern could help machine to find better sentences with different words that could only be       influenced by its previous or back words.\n",
    "     For parsing and pattern match, if we change semantic of the set that machine has to parse or match, then machine could have found wrong pattern for its created results. Also, we have to make potential patterns for one set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. What's the Language Model;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans: Finding a probability distribution that can show the probability of a sentence or a sequence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. Can you came up with some sceneraies at which we could use Language Model?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans: Machine transformation;talking between human and machine;ARS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7. What's the 1-gram language model;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans:One word probability only depends on itself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8. What's the disadvantages and advantages of 1-gram language model;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans: It use only 1 word to calculate probability. Simply understanding and using for advantages, and difficultly organising better sentence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9. What't the 2-gram models;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans: Mainly using 2 words to canculate joint distribution between those 2 words. Also, one word can only be influenced by previous one word, not behind one. This is one of markoff process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 编程实践部分"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. 设计你自己的句子生成器"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如何生成句子是一个很经典的问题，从1940s开始，图灵提出机器智能的时候，就使用的是人类能不能流畅和计算机进行对话。和计算机对话的一个前提是，计算机能够生成语言。\n",
    "\n",
    "计算机如何能生成语言是一个经典但是又很复杂的问题。 我们课程上为大家介绍的是一种基于规则（Rule Based）的生成方法。该方法虽然提出的时间早，但是现在依然在很多地方能够大显身手。值得说明的是，现在很多很实用的算法，都是很久之前提出的，例如，二分查找提出与1940s, Dijstra算法提出于1960s 等等。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在著名的电视剧，电影《西部世界》中，这些机器人们语言生成的方法就是使用的SyntaxTree生成语言的方法。\n",
    "\n",
    "> \n",
    ">\n",
    "\n",
    "![WstWorld](https://timgsa.baidu.com/timg?image&quality=80&size=b10000_10000&sec=1561818705&di=95ca9ff2ff37fcb88ae47b82c7079feb&src=http://s7.sinaimg.cn/mw690/006BKUGwzy75VK46FMi66&690)\n",
    "\n",
    "> \n",
    ">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在这一部分，需要各位同学首先定义自己的语言。 大家可以先想一个应用场景，然后在这个场景下，定义语法。例如：\n",
    "\n",
    "在西部世界里，一个”人类“的语言可以定义为：\n",
    "``` \n",
    "human = \"\"\"\n",
    "human = 自己 寻找 活动\n",
    "自己 = 我 | 俺 | 我们 \n",
    "寻找 = 看看 | 找找 | 想找点\n",
    "活动 = 乐子 | 玩的\n",
    "\"\"\"\n",
    "```\n",
    "\n",
    "一个“接待员”的语言可以定义为\n",
    "```\n",
    "host = \"\"\"\n",
    "host = 寒暄 报数 询问 业务相关 结尾 \n",
    "报数 = 我是 数字 号 ,\n",
    "数字 = 单个数字 | 数字 单个数字 \n",
    "单个数字 = 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9 \n",
    "寒暄 = 称谓 打招呼 | 打招呼\n",
    "称谓 = 人称 ,\n",
    "人称 = 先生 | 女士 | 小朋友\n",
    "打招呼 = 你好 | 您好 \n",
    "询问 = 请问你要 | 您需要\n",
    "业务相关 = 玩玩 具体业务\n",
    "玩玩 = 耍一耍 | 玩一玩\n",
    "具体业务 = 喝酒 | 打牌 | 打猎 | 赌博\n",
    "结尾 = 吗？\"\"\"\n",
    "\n",
    "```\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "请定义你自己的语法: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "第一个语法："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = '''\n",
    "client=自己 搜寻 业务\n",
    "自己=我们|我|我的家庭\n",
    "搜寻=需求|寻找|想要|想找\n",
    "业务=手机业务|上网业务|彩信业务|银行业务\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "第二个语法："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "server = '''\n",
    "server=问好 报工号 询问 转接部门 再次询问 结束\n",
    "问好=先生您好,|女士您好,|小朋友您好,\n",
    "报工号=我是工号 数字 ,|这里是工号 数字 ,\n",
    "数字=单个数字 | 数字 单个数字\n",
    "单个数字=1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9 \n",
    "询问=请问您需要咨询什么? | 您是否需要咨询业务?\n",
    "转接部门=转接用词 部门类别\n",
    "转接用词=请您稍等,帮你转接到|了解了,请您稍等,给您转接到\n",
    "部门类别=手机部门, | 互联网部门, | 彩信部门, | 网银部门, \n",
    "再次询问=您好, 再咨询\n",
    "再咨询=还需要别的咨询服务吗?|还有别的需求吗?\n",
    "结束=祝您生活愉快,再见!|谢谢您的来电,再见!|期待您的再次来电,再见!\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: 然后，使用自己之前定义的generate函数，使用此函数生成句子。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: 然后，定义一个函数，generate_n，将generate扩展，使其能够生成n个句子:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # np.random.choice只能从一唯数据中提取数据\n",
    "import random\n",
    "def create_grammar(grammar_str, split='=>', line_split='\\n'):\n",
    "    grammar = {}\n",
    "    for line in grammar_str.split(line_split):\n",
    "        if not line.strip(): \n",
    "            continue\n",
    "        exp, stmt = line.split(split)\n",
    "        grammar[exp.strip()] = [s.split() for s in stmt.split('|')]\n",
    "    return grammar\n",
    "gram=create_grammar(server,split='=')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'女士您好,这里是工号796,请问您需要咨询什么?请您稍等,帮你转接到网银部门,您好,还需要别的咨询服务吗?谢谢您的来电,再见!'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def generate(string,target):\n",
    "    if target not in gram:\n",
    "        return target\n",
    "    sen=[generate(gram,t) for t in random.choice(string[target])]\n",
    "    return ''.join(e for e in sen if e)\n",
    "generate(gram,target='server')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "女士您好,这里是工号6,您是否需要咨询业务?了解了,请您稍等,给您转接到手机部门,您好,还需要别的咨询服务吗?祝您生活愉快,再见!\n",
      "小朋友您好,这里是工号765,您是否需要咨询业务?请您稍等,帮你转接到互联网部门,您好,还需要别的咨询服务吗?谢谢您的来电,再见!\n",
      "先生您好,我是工号84,您是否需要咨询业务?请您稍等,帮你转接到手机部门,您好,还需要别的咨询服务吗?期待您的再次来电,再见!\n",
      "先生您好,这里是工号85,您是否需要咨询业务?了解了,请您稍等,给您转接到手机部门,您好,还有别的需求吗?祝您生活愉快,再见!\n",
      "小朋友您好,我是工号43,请问您需要咨询什么?了解了,请您稍等,给您转接到网银部门,您好,还需要别的咨询服务吗?祝您生活愉快,再见!\n",
      "女士您好,我是工号1,请问您需要咨询什么?了解了,请您稍等,给您转接到手机部门,您好,还需要别的咨询服务吗?期待您的再次来电,再见!\n",
      "先生您好,这里是工号16,请问您需要咨询什么?了解了,请您稍等,给您转接到手机部门,您好,还有别的需求吗?祝您生活愉快,再见!\n",
      "女士您好,这里是工号3,请问您需要咨询什么?了解了,请您稍等,给您转接到互联网部门,您好,还有别的需求吗?谢谢您的来电,再见!\n",
      "女士您好,这里是工号5,您是否需要咨询业务?了解了,请您稍等,给您转接到彩信部门,您好,还需要别的咨询服务吗?谢谢您的来电,再见!\n",
      "小朋友您好,我是工号9,您是否需要咨询业务?请您稍等,帮你转接到彩信部门,您好,还需要别的咨询服务吗?谢谢您的来电,再见!\n",
      "先生您好,这里是工号73,请问您需要咨询什么?请您稍等,帮你转接到互联网部门,您好,还有别的需求吗?谢谢您的来电,再见!\n",
      "先生您好,我是工号92,您是否需要咨询业务?了解了,请您稍等,给您转接到彩信部门,您好,还有别的需求吗?期待您的再次来电,再见!\n",
      "先生您好,我是工号691,您是否需要咨询业务?请您稍等,帮你转接到互联网部门,您好,还有别的需求吗?祝您生活愉快,再见!\n",
      "女士您好,我是工号1664,您是否需要咨询业务?了解了,请您稍等,给您转接到手机部门,您好,还有别的需求吗?谢谢您的来电,再见!\n",
      "女士您好,我是工号2,请问您需要咨询什么?了解了,请您稍等,给您转接到手机部门,您好,还有别的需求吗?谢谢您的来电,再见!\n",
      "先生您好,这里是工号5,请问您需要咨询什么?请您稍等,帮你转接到手机部门,您好,还需要别的咨询服务吗?期待您的再次来电,再见!\n",
      "女士您好,我是工号71,请问您需要咨询什么?请您稍等,帮你转接到手机部门,您好,还有别的需求吗?期待您的再次来电,再见!\n",
      "女士您好,这里是工号78,您是否需要咨询业务?请您稍等,帮你转接到彩信部门,您好,还需要别的咨询服务吗?谢谢您的来电,再见!\n",
      "小朋友您好,我是工号6,请问您需要咨询什么?了解了,请您稍等,给您转接到手机部门,您好,还有别的需求吗?谢谢您的来电,再见!\n",
      "女士您好,我是工号61,您是否需要咨询业务?请您稍等,帮你转接到互联网部门,您好,还有别的需求吗?期待您的再次来电,再见!\n"
     ]
    }
   ],
   "source": [
    "def generate_n(n,grammar):\n",
    "    for i in range(n):\n",
    "        print(generate(create_grammar(grammar,split='='),target='server'))\n",
    "    return\n",
    "generate_n(20,server)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. 使用新数据源完成语言模型的训练"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "按照我们上文中定义的`prob_2`函数，我们更换一个文本数据源，获得新的Language Model:\n",
    "\n",
    "1. 下载文本数据集（你可以在以下数据集中任选一个，也可以两个都使用）\n",
    "    + 可选数据集1，保险行业问询对话集： https://github.com/Computing-Intelligence/insuranceqa-corpus-zh/raw/release/corpus/pool/train.txt.gz\n",
    "    + 可选数据集2：豆瓣评论数据集：https://github.com/Computing-Intelligence/datasource/raw/master/movie_comments.csv\n",
    "2. 修改代码，获得新的**2-gram**语言模型\n",
    "    + 进行文本清洗，获得所有的纯文本\n",
    "    + 将这些文本进行切词\n",
    "    + 送入之前定义的语言模型中，判断文本的合理程度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Loading model cost 0.744 seconds.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "读取完成\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Prefix dict has been built succesfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.2926744140953218e-05\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import jieba\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "class Gram2:\n",
    "    def __init__(self,path,save_path):\n",
    "        self._path=path\n",
    "        self._save_path=save_path\n",
    "        self._clean_data()\n",
    "            \n",
    "    def _clean_data(self):\n",
    "        if self._path:\n",
    "            context=[]\n",
    "            with open(path,'r') as fp:\n",
    "                if fp:\n",
    "                    while 1:\n",
    "                        line=fp.readline()\n",
    "                        if not line:\n",
    "                            print('读取完成')\n",
    "                            break\n",
    "                        context+=line.strip().split('\\n')\n",
    "            chinese_words=[str(context[x].split('++$++')[-2]).strip()\\\n",
    "                           for x in range(len(context))]\n",
    "            with open(self._save_path,'w',encoding='utf8') as fp:\n",
    "                if fp:\n",
    "                    for line in chinese_words:\n",
    "                        fp.write(line+'\\n')\n",
    "    \n",
    "    def _get_2gram_token(self):\n",
    "        TOKEN=[]       \n",
    "        with open(self._save_path,'r') as fp:\n",
    "            if fp:\n",
    "                while 1:\n",
    "                    line=fp.readline()\n",
    "                    if not line:\n",
    "                        break\n",
    "                    TOKEN+=list(jieba.cut(line.strip()))\n",
    "        TOKEN=[x for x in TOKEN if x!='？']\n",
    "        TOKEN_2_GRAM = [''.join(TOKEN[i:i+2]) for i in range(len(TOKEN[:-2]))]\n",
    "        return TOKEN,TOKEN_2_GRAM\n",
    "        \n",
    "    def prob_2(self,word1, word2):\n",
    "        TOKEN,TOKEN_2_GRAM=self._get_2gram_token()\n",
    "        words_count=Counter(TOKEN_2_GRAM)\n",
    "        if word1 + word2 in words_count: \n",
    "            return words_count[word1+word2] / len(TOKEN_2_GRAM)\n",
    "        else:\n",
    "            return 1 / len(TOKEN_2_GRAM)\n",
    "            \n",
    "    \n",
    "if __name__=='__main__':\n",
    "    path='/home/abc/下载/lesson1+assessment1/train.txt'\n",
    "    save_path='/home/abc/chinese_word.txt'\n",
    "    gram2=Gram2(path,save_path=save_path)\n",
    "    #TOKEN=gram2.get_data()\n",
    "    print(gram2.prob_2('我想买','保险'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. 获得最优质的的语言"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "当我们能够生成随机的语言并且能判断之后，我们就可以生成更加合理的语言了。请定义 generate_best 函数，该函数输入一个语法 + 语言模型，能够生成**n**个句子，并能选择一个最合理的句子: \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "提示，要实现这个函数，你需要Python的sorted函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "通过随机句子,我们可以得到以最高概率0.00154%,得到了句子是:\n",
      "那边的被人吓死的特朗普抚摸那边的马桶\n",
      "--------------------------------------------------\n",
      "产生地全部随机句子如下:\n",
      "那边的被人吓死的特朗普抚摸那边的马桶\n",
      "那边的落到粪坑里的被人吓死的落到粪坑里的马桶吃那边的被人吓死的马克龙\n",
      "这边的落到粪坑里的被人吓死的被人吓死的大便不出的被人吓死的大便不出的被人吓死的马克龙注视那边的狗屎\n",
      "这边的狗屎吃那边的特朗普\n",
      "这边的狗屎尝试那边的特朗普\n",
      "那边的大便不出的马克龙抚摸那边的大便不出的马克龙\n",
      "这边的落到粪坑里的大便不出的被人吓死的特朗普抚摸那边的特朗普\n",
      "那边的落到粪坑里的被人吓死的狗屎抚摸那边的被人吓死的被人吓死的落到粪坑里的大便不出的特朗普\n",
      "那边的落到粪坑里的落到粪坑里的马克龙尝试这边的被人吓死的马桶\n",
      "这边的落到粪坑里的马桶尝试这边的落到粪坑里的马桶\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import jieba\n",
    "import random\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "\n",
    "my_own_grammar_2= \"\"\"\n",
    "my_own_grammar_2 => noun_phrase verb_phrase\n",
    "noun_phrase => Article Adj* noun\n",
    "Adj* => null | Adj Adj*\n",
    "verb_phrase => verb noun_phrase\n",
    "Article =>  这边的 | 那边的\n",
    "noun =>   特朗普 |  马克龙 | 马桶 | 狗屎\n",
    "verb => 注视   |  抚摸 |  吃 | 尝试\n",
    "Adj =>  落到粪坑里的  | 大便不出的 | 被人吓死的\n",
    "\"\"\"\n",
    "\n",
    "class NewGram:\n",
    "    def __init__(self,structure,target):\n",
    "        self._structure=structure\n",
    "        self._target=target\n",
    "\n",
    "    def _create_grammar(self,sentence,split='=>', line_split='\\n'):\n",
    "        grammar1 = {}\n",
    "        for line in sentence.split(line_split):\n",
    "            if not line.strip(): \n",
    "                continue\n",
    "            exp, stmt = line.split(split)\n",
    "            grammar1[exp.strip()] = [s.split() for s in stmt.split('|')]\n",
    "        return grammar1\n",
    "    \n",
    "    def _generate(self,grammar,target):\n",
    "        if target not in grammar:\n",
    "            return target\n",
    "        sen=[self._generate(grammar,t) for t in random.choice(grammar[target])]\n",
    "        return ''.join([e if e != '/n' else '\\n' for e in sen if e != 'null'])\n",
    "    \n",
    "    \n",
    "    def _get_token(self,n):\n",
    "        self._grammar=self._create_grammar(self._structure)\n",
    "        total_sentences=str()\n",
    "        self.TOKEN=[]\n",
    "        for i in range(n):\n",
    "            total_sentences+=(self._generate(self._grammar,target=self._target))\n",
    "            self.TOKEN+=list(jieba.cut(total_sentences))\n",
    "        self.words_count=Counter(self.TOKEN)\n",
    "        self.TOKEN_2_GRAM=[''.join(self.TOKEN[i:i+2]) for i in range(len(self.TOKEN[:-2]))]\n",
    "        self.words_count_2 = Counter(self.TOKEN_2_GRAM)\n",
    "        # print(self.TOKEN_2_GRAM)\n",
    "        \n",
    "    def _get_probablity(self,sentence):\n",
    "        words = list(jieba.cut(sentence))\n",
    "        sentence_pro = 1\n",
    "    \n",
    "        for i, word in enumerate(words[:-1]):\n",
    "            next_ = words[i+1]\n",
    "#             print(self.words_count_2)\n",
    "#             print()\n",
    "#             print(self.TOKEN_2_GRAM)\n",
    "#             print(word)\n",
    "            prob1=self._prob_1(word)\n",
    "            if not prob1:\n",
    "                print('当前有分词出现概率为0!')\n",
    "                print(word)\n",
    "                return prob1\n",
    "            probability = self._prob_2(word, next_)/self._prob_1(word)\n",
    "            \n",
    "        \n",
    "            sentence_pro *= probability\n",
    "        return sentence_pro\n",
    "    \n",
    "    def _prob_1(self,word): \n",
    "        return self.words_count[word] / len(self.TOKEN)\n",
    "    \n",
    "    def _prob_2(self,word1, word2): # 计算两个词的联合概率\n",
    "        if word1 + word2 in self.words_count_2: # 加法平滑\n",
    "            return (1+self.words_count_2[word1+word2]) / (len(self.TOKEN)+self.words_count[word1])\n",
    "        else:\n",
    "            return 1 / len(self.TOKEN_2_GRAM)\n",
    "    \n",
    "    def generate_best(self,n):\n",
    "        self._get_token(n)\n",
    "        final_rate=dict()\n",
    "        for sen in [self._generate(self._grammar,self._target) for i in range(n)]:\n",
    "            final_rate[sen]=self._get_probablity(sen)\n",
    "        return max(sorted(final_rate.values(),reverse=True)),max(sorted(final_rate,reverse=True)),final_rate.keys()\n",
    "        \n",
    "if __name__=='__main__':\n",
    "    model=NewGram(my_own_grammar_2,target='my_own_grammar_2')\n",
    "    #n_sen=model.generate_n(20,target='my_own_grammar')\n",
    "    #print(n_sen)\n",
    "    # TOKEN_2_GRAM=model.get_token(20,target='my_own_grammar')\n",
    "    rate,key,total_sen=model.generate_best(10)\n",
    "    print('通过随机句子,我们可以得到以最高概率%0.5f%%,得到了句子是:\\n%s'%(rate,key))\n",
    "    print('-'*50)\n",
    "    print('产生地全部随机句子如下:')\n",
    "    for sen in total_sen:\n",
    "        print(sen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "好了，现在我们实现了自己的第一个AI模型，这个模型能够生成比较接近于人类的语言。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q: 这个模型有什么问题？ 你准备如何提升？ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans:经过训练集训练后的模型对于外部测试机数据敏感度过低,无法有效地抓取真正意义上'人类的语言'.可以通过提升语法数据集量来训练模型,让其能认知更多的中文语法.另外,此模型由于使用jieba库分词,故无法对英语语法进行有效地训练.另外,如果个别单词出现概率为0.除了加入更多词和语法训练外,可以考虑针对条件概率下的平滑处理."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 以下内容为可选部分，对于绝大多数同学，能完成以上的项目已经很优秀了，下边的内容如果你还有精力可以试试，但不是必须的。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. (Optional) 完成基于Pattern Match的语句问答\n",
    "> 我们的GitHub仓库中，有一个assignment-01-optional-pattern-match，这个难度较大，感兴趣的同学可以挑战一下。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### 5. (Optional) 完成阿兰图灵机器智能原始论文的阅读\n",
    "1. 请阅读阿兰图灵关于机器智能的原始论文：https://github.com/Computing-Intelligence/References/blob/master/AI%20%26%20Machine%20Learning/Computer%20Machinery%20and%20Intelligence.pdf \n",
    "2. 并按照GitHub仓库中的论文阅读模板，填写完毕后发送给我: mqgao@kaikeba.com 谢谢"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "各位同学，我们已经完成了自己的第一个AI模型，大家对人工智能可能已经有了一些感觉，人工智能的核心就是，我们如何设计一个模型、程序，在外部的输入变化的时候，我们的程序不变，依然能够解决问题。人工智能是一个很大的领域，目前大家所熟知的深度学习只是其中一小部分，之后也肯定会有更多的方法提出来，但是大家知道人工智能的目标，就知道了之后进步的方向。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "然后，希望大家对AI不要有恐惧感，这个并不难，大家加油！"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://timgsa.baidu.com/timg?image&quality=80&size=b9999_10000&sec=1561828422005&di=48d19c16afb6acc9180183a6116088ac&imgtype=0&src=http%3A%2F%2Fb-ssl.duitang.com%2Fuploads%2Fitem%2F201807%2F28%2F20180728150843_BECNF.thumb.224_0.jpeg)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
